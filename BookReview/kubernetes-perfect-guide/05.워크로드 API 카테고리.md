# 5장 워크로드 API 카테고리

# 5.1 워크로드 API 카테고리 개요

<img width="1003" alt="스크린샷 2024-08-18 오후 9 35 08" src="https://github.com/user-attachments/assets/7fdb9525-1266-4859-961b-1397daec37b4">

쿠버네티스 리소스는 크게 다섯 가지 카테고리로 분류되어 있는데 그 중 **워크로드 API 카테고리로 분류된 리소스는 클러스터에 컨테이너를 기동시키기 위해 사용되는 리소스**이다. 내부에서 사용되는 리소스를 제외하고, 사용자가 직접 사용하는 리소스는 총 여덟 가지다.

- 파드
- 레플리케이션 컨트롤러
- 레플리카셋
- 디플로이먼트
- 데몬셋
- 스테이트풀셋
- 잡
- 크론잡

파드를 최소 단위로 하여 그것을 관리하는 상위 리소스가 있는 부모 자식 관계로 되어 있다. 예를 들어, 디플로이먼트는 레플리카셋을 관리하고 레플리카셋은 파드를 관리한다.

# 파드(Pod)

<img width="727" alt="2" src="https://github.com/user-attachments/assets/971cafab-8933-4288-a718-17cb452eacd8">

워크로드 리소스의 최소 단위는 `파드(Pod)`라고 불리는 리소스다. 파드는 한 개 이상의 컨테이너로 구성되며, 같은 파드에 포함된 컨테이너끼리는 네트워크적으로 격리되어 있지 않고 IP 주소를 공유한다. 따라서 파드 내부의 컨테이너는 서로 localhost로 통신할 수 있다.

대부분의 경우 하나의 파드에 하나의 컨테이너를 가지지만 메인 컨테이너 이외에 프록시 역할을 하는 컨테이너, 설정값을 동적으로 변경시키는 컨테이너, 로컬 캐시용 컨테이너, SSL용 컨테이너 등 서브 컨테이너도 포함하여 여러 컨테이너를 가질 수도 있다. 하지만 nginx 컨테이너와 redis 컨테이너와 같은 메인 컨테이너를 하나의 파드 안에 두는 구성은 개별 파드의 이동이 어려워지고 복잡해지기 때문에 추천하지 않는다.

## 5.2.1 파드 디자인 패턴

파드 디자인 패턴에는 크게 세 종류가 있다. 이렇게 패턴이 구분되어 있지만, 보조적인 역할을 하는 서브 컨테이너를 통틀어 사이드카라고 부르는 경우도 있다.

### 사이드카 패턴(sidecar pattern)

<img width="744" alt="3" src="https://github.com/user-attachments/assets/bba28274-0eda-4a81-b22d-e231c22b1de9">

사이드카 패턴은 **메인 컨테이너 외에 보조적인 기능을 추가**하는 서브 컨테이너를 포함하는 패턴이다. 위 그림처럼 특정 변경 사항을 감지하여 동적으로 설정을 변경하는 컨테이너, 깃 저장소와 로컬 스토리지를 동기화하는 컨테이너, 애플리케이션의 로그 파일을 오브젝트 스토리지로 전송하는 컨테이너라는 구성이 자주 사용된다.

파드는 데이터 영역을 공유하고 가지고 있을 수 있기 때문에 대부분 데이터와 설정에 관련된 패턴이라 할 수 있다.

### 앰배서더 패턴(ambassador pattern)

<img width="856" alt="4" src="https://github.com/user-attachments/assets/5f6e27e5-f927-4926-95b4-b643c6a1a02d">

앰배서더 패턴은 메인 컨테이너가 외부 시스템과 접속할 때 대리로 중계해주는 서브 컨테이너(앰배서더 컨테이너)를 포함한 패턴이다. 파드에 두 개의 컨테이너가 있어 메인 컨테이너에 목적지에 localhost를 지정하여 앰배서더 컨테이너로 접속할 수 있다.

앰배서더 컨테이너를 사용하지 않고 메인 컨테이너에서 샤딩(sharding)된 데이터베이스 하나를 선택하여 접속하게 된다면 메인 컨테이너는 데이터베이스와의 결합도가 강해진다. 그러나 앰배서더 컨테이너를 사용함으로써 메인 컨테이너에서는 항상 localhost를 지정하여 앰배서더 컨테이너로만 접속하고 앰배서더 컨테이너가 여러 목적지에 중계하여 연결하도록 구성하면 느슨한 결합을 유지할 수 있다.

### 어댑터 패턴(adapter pattern)

<img width="857" alt="5" src="https://github.com/user-attachments/assets/035f4d51-8a6d-4136-a54d-6c13e42ac0d8">

어댑터 패턴은 서로 다른 데이터 형식을 변환해주는 컨테이너(어댑터 패턴)를 포함하는 패턴이다. 예를 들어, 프로메테우스(Prometheus) 등의 모니터링 소프트웨어에서는 정의된 형식으로 매트릭을 수집해야 한다. 그러나 대부분의 미들웨어가 제공하는 메트릭 출력 형식은 프로메테우스 메트릭 형식을 지원하지 않는다. 따라서 이러한 경우 어댑터 컨테이너를 사용하면 외부 요청에 맞게 데이터 형식으로 변환하고 데이터를 반환해준다. 어댑터 패턴의 경우도 메인 컨테이너와 어댑터 컨테이너 간에는 localhost를 통해 접속할 수 있다.

## 5.2.2 파드 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
	name: sample-pod
spec:
	containers:
	- name: nginx-container
		image: nginx:1.16
```

```yaml
# 파드 생성
kubectl apply -f sample-pod.yaml

# 파드 목록 표시
kubectl get pods

# 파드 상세 정보 표시
kubectl get pods --output wide
```

sample-pod 내부에 `nginx:1.16` 이미지를 사용한 컨테이너 하나를 기동하고 80/TCP 포트를 바인드하는 단순한 파드이다.

## 5.2.3 두 개의 컨테이너를 포함한 파드 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
	name: sample-pod
spec:
	containers:
	- name: nginx-container
		image: nginx:1.16
	- name: redis-container
		image: redis:3.2
```

위 매니페스트는 nginx와 redis라는 두 개의 컨테이너를 가진 파드 예제이다. 레디스(redis)sms 6379/TCP 포트를 바인드한다. 물론, nginx나 redis와 같은 메인 컨테이너를 하나의 파드 안에 같이 구성하는 것은 추천하지 않는다.

### 같은 포트를 사용하는 두 개의 컨테이너를 가진 파드 예제

```yaml
apiVersion: v1
kind: Pod
metadata:
	name: sample-pod-fail
spec:
	containers:
	- name: nginx-container-112
		image: nginx:1.16
	- name: nginx-container-113
		image: nginx:1.17
```

```yaml
# 포트 충돌이 있는 파드 생성
kubectl apply -f sample-pod-fail.yaml

# 파드 상태가 에러임을 확인
kubectl get pods

# 파드 로그 확인(여러 컨테이너가 있는 경우 특정 컨테이너 지정 가능)
kubectl logs sample-pod-fail -c nginx-container-113
```

<img width="1091" alt="6" src="https://github.com/user-attachments/assets/c332a903-eb8c-4c2e-b751-35af0ec0495b">

<img width="760" alt="7" src="https://github.com/user-attachments/assets/f471caf3-ed3a-4e2f-b997-6dbd0a900609">

파드는 네트워크 네임스페이스를 공유하고 있으므로, 일반 VM상에 80/TCP 포트를 바인드하는 서비스를 하나 이상 사용할 수 없는 환경과 같다고 보면 된다. 파드 내에서 containerPort가 충돌하지 않도록 해야 한다.

## 5.2.4 컨테이너 로그인과 명령어 실행

컨테이너 로그인이라고 하는 것은 실제 ssh와 같이 컨테이너에 직접 로그인하는 것이 아닌 가상 터미널을 생성(`-t`)하고, 표준 입력을 패스 스루(`-i`)하면서 `/bin/sh`를 실행하면 마치 컨테이너에 ssh로 로그인한 상태가 된다. 실제로 컨테이너에 로그인하여 확인하려면 `kubectl exec`에서 bash를 실행한다.

```bash
# 컨테이너에서 /bin/bash 실행
kubectl exec -it sample-pod -- /bin/bash
root@sample-pod:/# (이후부터 컨테이너 내부에서 명령어 실행 가능)

# 확인 작업에 필요한 패키지 설치
apt update && apt -y install iproute2 procps

# 컨테이너 내부에서 바인드(listen)하는 포트 확인
ss -napt | grep LISTEN

# 컨테이너 내부에서 프로세스 확인
ps aux
```

<img width="998" alt="8" src="https://github.com/user-attachments/assets/28743094-383c-4747-a136-90df20a397cc">

이외에도 `kubectl exec -it sample-pod --some-command`처럼 여러 가지 명령어를 실행할 수 있다.

```bash
# 컨테이너에서 ls 명령어 실행
kubectl exec -it sample-pod -- /bin/ls

# 다수의 컨테이너를 포함한 파드의 경우 특정 컨테이너 지정 가능
kubectl exec -it sample-2pod -c nginx-container -- /bin/ls

# 옵션을 포함한 명령어 실행
kubectl exec -it sample-pod -- /bin/ls --all --classify

# 파이프 등 특정 문자열을 포함한 경우의 실행
kubectl exec -it sample-pod -- /bin/bash -c "ls -all --classify | grep lib"
```

## 5.2.5 ENTRYPOINT 명령/CMD 명령과 command/args

```yaml
apiVersion: v1
kind: Pod
metatdata:
	name: sample-entrypoint
spec:
	containers:
	- name: nginx-container-112
		image: nginx:1.16
		command: ["/bin/sleep"] # ENTRYPOINT 명령으로 대체
		args: ["3600"] # CMD 명령으로 대체
```

쿠버네티스에서는 도커용 용어와 조금 다르게 ENTRYPOINT를 `command`, CMD를 `args`라고 부른다. 컨테이너를 실행할 때 도커 이미지의 ENTRYPOINT와 CMD를 덮어 쓰기하려면 파드 정의 내용 중 spec.containers[].command와 spec.containers[].args를 지정한다.

## 5.2.6 파드명 제한

쿠버네티스에서는 파드명에 제한이 있다. 파드명은 RFC1123의 호스트명 규약을 따르고 있어 다음 조건에 맞아야 한다.

- 이용 가능한 문자는 영어 소문자와 숫자
- 이용 가능한 기호는 `-` 또는 `.`
- 시작과 끝은 영문 소문자

## 5.2.7 호스트의 네트워크 구성을 사용한 파드 기동

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-pod
spec:
  hostNetwork: true
  containers:
    - name: nginx-container
      image: nginx:1.16
```

쿠버네티스에 기동하는 파드에 할당된 IP 주소는 쿠버네티스 노드의 호스트 IP 주소와 범위가 달라 외부에서 볼 수 없는 IP 주소가 할당된다. 호스트의 네트워크를 사용하는 설정(spec.hostNetwork)을 활성화하면 호스트상에서 프로세스를 기동하는 것과 같은 네트워크 구성(IP 주소, DNS 설정, host 설정 등)으로 파드를 기동시킬 수 있다.

hostNetwork를 사용한 파드는 쿠버네티스 노드의 IP 주소를 사용하는 관계로 포트 번호 충돌을 방지하기 위해 기본적으로 사용하지 않고 NodePort 서비스 등으로 해결할 수 있는지 검토하는 것이 좋다.

```bash
# 파드의 IP 주소 확인
kubectl get pod sample-hostnetwork -o wide

# 파드가 기동 중인 노드의 IP 주소 확인
kubectl get node minikube -o wide

# 파드의 호스트명 확인
kubectl exec -it sample-hostnetwork -- hostname

# 파드의 DNS 설정 확인
kubectl exec -it sample-hostnetwork -- cat /etc/resolv.conf
```

<img width="1220" alt="9" src="https://github.com/user-attachments/assets/268159ae-e0fa-4c21-a3ed-c679e7b11847">

## 5.2.8 파드 DNS 설정과 서비스 디스커버리

DNS 서버에 관한 설정(dnsPolicy)은 파드 정의(spec.dnsPolicy)에 설정한다.

### ClusterFirst(기본값)

```bash
# 컨테이너 내부의 DNS 설정 파일 /etc/resolv.conf를 표시
kubectl exec -it sample-dnspolicy-clusterfirst -- cat /etc/resolv.conf

# 클러스터 내부의 DNS Service에 할당된 IP 주소를 확인
kubectl get services -n kube-system
```

일반적으로 파드는 클러스터 내부 DNS를 사용하여 이름을 해석한다. 이는 서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용하기 위해서다. dnsPolicy가 ClusterFirst인 경우 클러스터 내부의 DNS 서버에 질의를 하고, 클러스터 내부 DNS에서 해석이 안 되는 도메인에 대해서는 업스트림 DNS 서버에 질의한다.

<img width="1273" alt="1" src="https://github.com/user-attachments/assets/4976ab4e-d844-4814-8838-5b47d940ae4c">

### None

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-dnspolicy-none
spec:
  dnsPolicy: None
  dnsConfig:
    nameservers:
    - 8.8.8.8
    - 8.8.4.4
    searches:
    - example.com
    options:
    - name: ndots
      value: "5"
  containers:
    - name: nginx-container
      image: nginx:1.16
```

Nond은 파드 정의 내에서 정적으로 설정한다. 특별한 요건에 따라서는 클러스터 외부 DNS 서버를 참조하는 경우도 있다. DNS 서버를 수동으로 설정하려면 spec.dnsPolicy: None이라고 설정한 후 dnsConfig에 설정하고 싶은 값을 작성하면 된다. 정적으로 외부 DNS 서버만 설정하면 클러스터 내부 DNS를 사용한 서비스 디스커버리는 사용할 수 없으므로 주의해야 한다.

<img width="1215" alt="2" src="https://github.com/user-attachments/assets/4b386554-22bf-41cf-b35c-7768fb036417">

### Default

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-dnspolicy-default
spec:
  dnsPolicy: Default
  containers:
    - name: nginx-container
      image: nginx:1.16
```

쿠버네티스 노드의 DNS 설정을 그대로 상속받는 경우에 사용한다. 파드가 기동하는 쿠버네티스 노드의 `/etc/resolv.conf`를 상속받는다. 위와 같이 파드를 생성하면 쿠버네티스 노드의 `/etc/resolv.conf`와 똑같은 내용을 확인할 수 있다. 쿠버네티스 노드의 DNS 설정을 상속받게 설정하면 클러스터 내부의 DNS를 사용한 서비스 디스커버리를 할 수 없게 되므로 주의해야 한다.

<img width="1253" alt="3" src="https://github.com/user-attachments/assets/7fd0c64f-488e-477e-bce1-530d030147c4">

### ClusterFirstWithHostNet

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-dnspolicy-clusterfirstwithhostnet
spec:
  dnsPolicy: ClusterFirstWithHostNet
  hostNetwork: true
  containers:
    - name: nginx-container
      image: nginx:1.16
```

hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우에 설정한다. hostNetwork를 사용하는 경우 기본값 Clusterfirst의 설정값은 무시되고 쿠버네티스 노드의 네트워크 설정(DNS 설정 포함)이 사용되기 때문에 명시적으로 ClusterFirstWithHostNet을 지정하도록 하자.

<img width="1548" alt="4" src="https://github.com/user-attachments/assets/785745fd-600f-43f0-8d9b-3c549cb6c00b">

## 5.2.9 정적 호스트명 해석 설정: /etc/hosts

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-hostaliases
spec:
  containers:
    - name: nginx-container
      image: nginx:1.16
    hostAliases:
    - ip: 8.8.8.8
      hostnames:
      - google-dns
      - google-public-dns
```

리눅스 운영체제에서는 DNS로 호스트명을 해석하기 전에 `/etc/hosts` 파일로 정적 호스트명을 해석한다. 쿠버네티스에서는 파드 내부 모든 컨테이너의 `/etc/hosts`를 변경하는 기능이 준비되어 있으며 `spec.hostAliases`로 지정하여 사용할 수 있다.

<img width="1327" alt="5" src="https://github.com/user-attachments/assets/ceed1f88-c243-462a-a5bc-bde77db900bb">

실제로 생성한 파드 내부의 `/etc/hosts`를 확인해보면 spec.ohstAliases에서 지정한 내용이 추가된 것을 확인할 수 있다.

## 5.2.10 작업 디렉터리 설정

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-hostaliases
spec:
  containers:
    - name: nginx-container
      image: nginx:1.16
    workingDir: /tmp
```

컨테이너에서 동작하는 애플리케이션의 작업 디렉터리는 도커 파일의 `WORKDIR` 명령 설정을 따르지만 `spec.containers[].workingDir`로 덮어 쓸 수도 있다. 예를 들어, 볼륨 기능을 사용하여 특정 스크립트 등이 배치된 볼륨을 파드에 마운트할 때 그 스크립트가 배치된 디렉터리로 이동한 후 실행하고 싶은 경우에 사용할 수 있다.

<img width="1216" alt="6" src="https://github.com/user-attachments/assets/5caf29b1-eed1-41c8-8d2d-114dc795913a">

# 5.3 레플리카셋(ReplicaSet)/레플리케이션 컨트롤러(ReplicationController)

<img width="772" alt="7" src="https://github.com/user-attachments/assets/0a99b6bf-fa49-43a8-9568-089d01cf4be4">

레플리카셋/레플리케이션 컨트롤러는 파드의 레플리카를 생성하고 지정한 파드 수를 유지하는 리소스다. 원래 파드의 레플리카를 생성하는 리소스의 이름은 레플리케이션 컨트롤러였는데, 시간이 지나 레플리카셋으로 이름이 변경되면서 일부 기능이 추가되었다. 레플리케이션 컨트롤러는 앞으로 사용하지 않는 추세이기 때문에 기본적으로 레플리카셋을 사용하자.

## 5.3.1 레플리카셋 생성

```yaml
apiVersion: v1
kind: ReplicaSet
metadata:
  name: sample-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
  containers:
    - name: nginx-container
      image: nginx:1.16
```

`spec.template` 부분에는 복제할 파드 정의(Pod Template)를 기술한다. 생성한 파드를 레플리카 수 3으로 확장시킨 레플리카셋을 생성한다.

```bash
# 레플리카셋 생성
kubectl apply -f sample-rs.yaml

# 레플리카셋 확인
kubectl get replicasets -o wide

# 레플리카셋이 파드 관리에 사용하는 레이블(app=sample-app)을 지정하여 파드 목록 표시
kubectl get pods -l app=sample-app
```

<img width="1294" alt="8" src="https://github.com/user-attachments/assets/9825ba13-e2d2-4580-a100-4e24e45a61e2">

레플리카셋을 확인해보면 세 개의 파드가 기동 중인 것을 확인할 수 있다. 실제로 레이블을 지정하여 파드를 확인해 봐도 기동 중인 세 개의 파드를 확인할 수 있다. 배포된 노드를 보면, 파드가 각각 다른 노드에 흩어져 생성되어 있기 때문에 만약 노드에 장애가 발생하더라도 서비스에 미치는 영향을 최소화할 수 있다.

또한, 레플리카셋이 생성하는 파드는 `레플리카셋 이름-임의의 문자열`로 명명된다.

## 5.3.2 파드 정지와 자동화된 복구

<img width="781" alt="9" src="https://github.com/user-attachments/assets/082a8d09-e03e-4258-8a95-18dce6cfa1f6">

레플리카셋에서는 노드나 파드에 장애가 발생했을 때 지정한 파드 수를 유지하기 위해 다른 노드에서 파드를 기동시켜 주기 때문에 장애 시에도 많은 영향을 받지 않는다. 이는 쿠버네티스의 중요한 콘셉트 중 하나로, `자동화된 복구`라는 기능이다.

```bash
# 파드 정지(삭제)
# 실제 기동 중인 파드명을 지정
kubectl delete pod sample-rs-g6cpz

# 레플리카셋 목록 표시
kubectl get pods -o wide

# 레플리카 상세 정보 표시
kubectl describe replicaset sample-rs
```

자동화된 복구의 동작을 확인하기 위해 테스트로 파드 한 대를 정지시키고, 다시 파드를 확인하면 즉시 레플리카셋에 의해 파드가 새로 생성되는 것을 확인할 수 있다.

레플리카셋의 파드 수 증갑 이력은 `kubectl describe rs` 명령어로 확인할 수 있다.

<img width="1231" alt="1" src="https://github.com/user-attachments/assets/4753341d-b2b0-4099-aca2-1d0702633957">

<img width="1247" alt="2" src="https://github.com/user-attachments/assets/d052242a-3308-4df1-8616-99724582382b">

## 5.3.3 레플리카셋과 레이블

<img width="877" alt="3" src="https://github.com/user-attachments/assets/0d65d5bd-5807-4ad8-a734-5b865413d8d7">
레플리카셋은 쿠버네티스가 파드를 모니터링하여 파드 수를 조정한다. 모니터링은 특정 레이블을 가진 파드 수를 계산하는 형태로 이루어진다. 레플리카 수가 부족한 경우 매니페스트에 기술된 `spec.template`로 파드를 생성하고 레플리카 수가 많은 경우 레이블이 일치하는 파드 중 하나를 삭제한다.

```yaml
selector:
	matchLabels:
		app: sample-app
template:
  metadata:
    labels:
      app: sample-app
```

어떤 레이블을 가진 파드를 계산할지는 다음과 같이 `spec.selector` 부분에 지정한다. `spec.template.metadata.labels` 부분에 해당하는 `app: sample-app` 레이블을 설정하고 `app: sample-app` 레이블이 지정된 상태에서 파드가 생성되었기 때문에 레플리카 수로 계산된다.

### selector와 template 불일치한 상태로 생성하려는 경우

<img width="984" alt="4" src="https://github.com/user-attachments/assets/961fd1f5-bc06-4b92-bf66-e7329c80d6c3">

```yaml
apiVersion: v1
kind: ReplicaSet
metadata:
  name: sample-rs-fail
spec:
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app-fail
  containers:
    - name: nginx-container
      image: nginx:1.16
```

`spec.selector`와 `spec.template.metadata.labels`의 레이블이 일치하지 않으면 어떻게 될까?

```yaml
# 레이블 불일치 상태로 생성
kubectl apply -f sample-rs-fail.yaml

# The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}:
# `selector` does not match template `labels`
```

레이블이 일치하지 않는 경우 에러가 발생하여 생성할 수 없게 되어있다. 만약 에러가 발생하지 않는다면 레플리카 수를 늘리려고 파드가 계속 생성될 것이다.

### 레플리카셋의 셀렉터와 일치하는 레이블을 가진 파드 생성하는 경우

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-rs
  lables:
    app: sample-app
containers:
  - name: nginx-container
    image: nginx:1.16
```

위와 같이 레플리카 수를 3으로 지정한 sample-rs.yaml과 같은 레이블(app: sample-app)을 가진 sample-rs-pod를 생성하고 테스트해보자.

```bash
kubectl apply -f sample-rs.yaml
kubectl apply -f sample-rs-pod.yaml
```

생성 후 파드의 상태를 확인해보자. `kubectl get pods -L` 옵션을 사용하면 각 파드에 지정된 레이블도 표시할 수 있다. 

<img width="994" alt="5" src="https://github.com/user-attachments/assets/c45ddb36-b4ab-49f4-b235-374f502552fe">

## 5.3.4 레플리카셋과 스케일링

### 매니페스트를 수정하여 kubectl apply -f 명령어를 실행

```bash
sed -i -e 's|replicas: 3|replicas: 4|' sample-rs.yaml
kubectl apply -f sample-rs.yaml
```

<img width="1364" alt="6" src="https://github.com/user-attachments/assets/0fc95ae6-90b7-4f1d-ac21-b5143b267a3c">

첫 번째 방법인 매니페스트를 수정하여 kubectl apply 명령어를 실행하는 방법인데 IaC(Infrastructure as Code)를 구현하기 위해서라도 이 방법을 사용하는 것이 좋다.

### kubectl scale 명령어를 사용하여 스케일 처리

두 번째 방법은 kubectl scale 명령어를 사용하여 스케일링하는 방법이다. scale 명령어를 사용한 스케일 처리는 레플리카 셋 이외에도 레플리케이션 컨트롤러/디플로이먼트/스테이트풀셋/잡/크론잡에서 사용할 수 있다.

```bash
# 레플리카 수를 5로 변경
kubectl scale replicaset sample-rs --replicas 5

# 레플리카셋 목록 표시
kubectl get replicats
```

<img width="1330" alt="7" src="https://github.com/user-attachments/assets/24999690-5e30-4406-a2c0-7bd3402d0414">

## 5.3.5 일치성 기준 조건과 집합성 기준 조건

레플리카 제어 조건은 서비스 중단 예정인 레플리케이션 컨트롤러의 일치성 기준(equality-based) 셀렉터였지만, 레플리카셋에서는 좀 더 강화된 집합성 기준(set-based) 셀렉터를 사용하여 유연한 제어도 가능하다. 쿠버네티스에서 어떤 조건을 지정할 때는 이 두 가지 방법이 있고 레플리카셋 외에 스케줄링할 때도 이 조건 지정이 사용된다.

- 일치성 기준
    - 조건부에 일치 불일치(`=`, `!=`) 조건 지정
- 집합성 기준
    - 조건부에 일치 불일치(`=`, `!=`) 조건 지정과 집합(`in`, `notin`, `exists`) 조건 지정 기능

일치성 기준 조건에서는 조건부에서 같은지 혹은 같지 않은지에 대한 조건을 사용할 수 있다. 예를 들어, `app=sample-app`과 같이 지정한다. 집합성 기준 조건에서는 일치성 기준 조건과 함께 집합 조건을 지정할 수 있다. 예를 들어, `env In [development, staging]`과 같이 지정할 수 있따. 또 `In` 연산자는 스케줄링 조건에서 사용할 때 수치 비교도 가능하다.